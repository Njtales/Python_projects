{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c84bc3aa",
   "metadata": {},
   "source": [
    "## Scrape wikipedia articles python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a3d9690",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T16:02:04.353791Z",
     "start_time": "2022-06-09T16:02:04.082882Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "response = requests.get(url=\"https://en.wikipedia.org/wiki/Web_scraping\",)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "title = soup.find(id=\"firstHeading\")\n",
    "print(title.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3915342a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T16:02:04.849824Z",
     "start_time": "2022-06-09T16:02:04.817826Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a href=\"/wiki/Blog_scraping\" title=\"Blog scraping\">Blog scraping</a>\n"
     ]
    }
   ],
   "source": [
    "# Get all the links\n",
    "allLinks = soup.find(id=\"bodyContent\").find_all(\"a\")\n",
    "random.shuffle(allLinks)\n",
    "linkToScrape = 0\n",
    "\n",
    "for link in allLinks:\n",
    "    # We are only interested in other wiki articles\n",
    "    if link['href'].find(\"/wiki/\") == -1: \n",
    "        continue\n",
    "\n",
    "    # Use this link to scrape\n",
    "    linkToScrape = link\n",
    "    break\n",
    "\n",
    "print(linkToScrape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2959a77d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T16:14:52.677347Z",
     "start_time": "2022-06-09T16:14:38.233634Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web scraping\n",
      "Link farm\n",
      "Inktomi\n",
      "Pay-per-click\n",
      "Web analytics\n",
      "Website localization\n",
      "Category:Communication design\n",
      "Category:Lithography\n",
      "Joseph Latour\n",
      "N. F. S. Grundtvig\n",
      "Evangelical Lutheran Hymnary\n",
      "Holy water\n",
      "Christian Church\n",
      "Template:Christianity sidebar\n",
      "Restorationism\n",
      "Jesus\n",
      "Christmas\n",
      "Cavalcade of Magi\n",
      "Patum de Berga\n",
      "Falles\n",
      "World Network of Biosphere Reserves in Europe and North America\n",
      "Swiss National Park\n",
      "Category:Articles containing French-language text\n",
      "1 Service Battalion\n",
      "Spruce Grove\n",
      "2006 Winter Olympics\n",
      "1980 Summer Olympics\n",
      "Nicaragua at the 1980 Summer Olympics\n",
      "Vietnam at the 1980 Summer Olympics\n",
      "Seychelles at the 1980 Summer Olympics\n",
      "Marc Larose\n",
      "File:Flag of Seychelles.svg\n",
      "Foreign relations of Russia\n",
      "Russia in the Council of Europe\n",
      "File:Implementation of European Court of Human Rights verdicts as of August 2021.svg\n",
      "Armenia in the Council of Europe\n",
      "Category:Armenian culture\n",
      "Armenian Catholic Archeparchy of Istanbul\n",
      "Armenian Catholic Church\n",
      "Mozarabic Rite\n",
      "Vatican Radio\n",
      "Radia Perlman\n",
      "Peter T. Kirstein\n",
      "Category:Articles with short description\n",
      "__ Connection refused\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Creating an endless scraper\n",
    "def scrapeWikiArticle(url):\n",
    "    response = requests.get(url=url,)\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    title = soup.find(id=\"firstHeading\")\n",
    "    print(title.text)\n",
    "\n",
    "    allLinks = soup.find(id=\"bodyContent\").find_all(\"a\")\n",
    "    random.shuffle(allLinks)\n",
    "    linkToScrape = 0\n",
    "\n",
    "    for link in allLinks:\n",
    "        # We are only interested in other wiki articles\n",
    "        if link['href'].find(\"/wiki/\") == -1: \n",
    "            continue\n",
    "\n",
    "        # Use this link to scrape\n",
    "        linkToScrape = link\n",
    "        break\n",
    "\n",
    "    scrapeWikiArticle(\"https://en.wikipedia.org\" + linkToScrape['href'])\n",
    "\n",
    "try:\n",
    "    scrapeWikiArticle(\"https://en.wikipedia.org/wiki/Web_scraping\")\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"__ Connection refused\")\n",
    "except KeyError:\n",
    "    print(\"__ Key Error\")\n",
    "except:\n",
    "    print(\"__ Something went wrong\")    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
